{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3580d1-1678-44ec-854c-1e9abe6c0227",
   "metadata": {},
   "source": [
    "## Preprocess input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d057f83c-4ac4-42de-9562-5365cdd7677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panflute as pf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c6049b-8e18-40a7-8944-f54724656782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(doc):\n",
    "    bad_blocks = [pf.BlockQuote]\n",
    "    filtered_doc = []\n",
    "    for _, x in enumerate(doc):\n",
    "        if type(x) in bad_blocks:\n",
    "            continue\n",
    "        markdown_output = pf.convert_text(x, input_format='panflute', output_format='markdown')\n",
    "        markdown_output_single_line = re.sub(r'([^\\n])\\n([^\\n])', r'\\1 \\2', markdown_output)\n",
    "        markdown_output_single_line = re.sub(r'  +', r' ', markdown_output_single_line)\n",
    "        filtered_doc.append(markdown_output_single_line)\n",
    "        # filtered_doc += markdown_output_single_line.split('. ')\n",
    "    return filtered_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534d04c-684e-4df2-b0d0-2f8eccc767de",
   "metadata": {},
   "source": [
    "Design choices\n",
    "\n",
    "I chose to use `max_chars` instead of `max_tokens`, because while tokenization schemes change (and probably should be replaced by pure character-level encoding), characters stay.\n",
    "\n",
    "Generally, setting `max_chars` to be about 1/4 of the total context window would be a safe choice. You can push up to 1/2 of the context window. More is not recommended, since the reply could be as long as the input.\n",
    "\n",
    "According to [Greg Kamradt's tests](https://twitter.com/GregKamradt/status/1722386725635580292), GPT-4 128k has perfect recall up to 64k context window, so we should be fine using 32k token inputs. For typical English text, that means something like 60k characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2865bc-6856-4f6d-811a-d645ba2c0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(paragraphs, max_chars):\n",
    "    \"\"\"\n",
    "    Split a list of paragraphs into chunks.\n",
    "\n",
    "    :param paragraphs: List of paragraphs (each paragraph is a string).\n",
    "    :param max_chars: Maximum number of characters allowed in a chunk.\n",
    "    :return: List of chunks (each chunk is a list of paragraphs).\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    current_length = 0\n",
    "    for para in paragraphs:\n",
    "        para_length = len(para)\n",
    "\n",
    "        # Check if the paragraph itself is too long\n",
    "        if para_length > max_chars:\n",
    "            raise ValueError(f\"Paragraph exceeds the maximum character limit: {para_length} characters\")\n",
    "\n",
    "        # Check if adding this paragraph would exceed the max length\n",
    "        if current_length + para_length <= max_chars:\n",
    "            current_chunk.append(para)\n",
    "            current_length += para_length\n",
    "        else:\n",
    "            # Start a new chunk\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [para]\n",
    "            current_length = para_length\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a784d2-00b5-41c1-9c45-42c9012264e5",
   "metadata": {},
   "source": [
    "## Make calls to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2307dac0-99b6-498d-b00e-71ccbc005fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "\n",
    "def gpt_proofread(text, model=\"gpt-4-1106-preview\"):\n",
    "    prompt = \"\"\"\n",
    "You are a proofreader. The user provides a piece of R-markdown, and you will proofread it. Do not say anything else.\n",
    "Do not simply perform an active-to-passive voice conversion. Do not improve the style. Restrict yourself to grammatical checks.\n",
    "You MUST reply in this format:\n",
    "\n",
    "a: <original sentence>\n",
    "b: <rewritten sentence>\n",
    "\n",
    "a: <original sentence>\n",
    "b: <rewritten sentence>\n",
    "\n",
    "...\n",
    "\n",
    "Reply only rewritten sentences. Skip all other sentences. Do not reply anything else. If the text requires no change, return an empty string.\n",
    "Do not use American-style quotation. Use logical quotation.\n",
    "\"\"\".strip()\n",
    "    example_user_1 = \"\"\"\n",
    "We use the convention putting derivative on the rows. This convention simplifies a lot of equations, and completely avoids transposing any matrix.\n",
    "\n",
    "In the next section, using the \"pebble construction,\" they studied \"Gamba perceptrons.\" They stated \"MLPs are essentially Gamba perceptrons.\"\n",
    "    \"\"\".strip()\n",
    "    example_assistant_1 = \"\"\"\n",
    "a: We use the convention putting derivative on the rows.\n",
    "b: We use the convention of putting the derivatives on the rows.\n",
    "\n",
    "a: In the next section, using the \"pebble construction,\" they studied \"Gamba perceptrons.\"\n",
    "b: In the next section, using the \"pebble construction\", they studied \"Gamba perceptrons\".\n",
    "\n",
    "a: They stated \"MLPs are essentially Gamba perceptrons.\"\n",
    "b: They stated \"MLPs are essentially Gamba perceptrons.\".\n",
    "    \"\"\"\n",
    "    example_user_2 = \"That is, for any $X \\subset R$, we have $\\psi(X)=\\psi(g(X))$.\"\n",
    "    example_assistant_2 = \"\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=model, \n",
    "                  messages=[{\"role\": \"system\", \"content\": prompt}, \n",
    "                            {\"role\": \"user\", \"content\": example_user_1},\n",
    "                            {\"role\": \"assistant\", \"content\": example_assistant_1},\n",
    "                            {\"role\": \"user\", \"content\": example_user_2},\n",
    "                            {\"role\": \"assistant\", \"content\": example_assistant_2},\n",
    "                            {\"role\": \"user\", \"content\": text}])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304c0cd-5d5b-4a01-b0e3-c586570041aa",
   "metadata": {},
   "source": [
    "## Postprocess output\n",
    "\n",
    "The most important problem we need to solve is fuzzy search, which might be necessary if the damned LLM does not return an exact string match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "243d0a6a-7bd4-4e70-95f7-c7fc9707abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzysearch import find_near_matches\n",
    "from warnings import warn\n",
    "\n",
    "def process_response(response, original_text=\"\", max_l_dist=10):\n",
    "    text = response.choices[0].message.content\n",
    "    # Check that the input sequence satisfies a certain format\n",
    "    prefix_list = [line.strip()[:2] for line in text.split('\\n') if line.strip() != \"\"]\n",
    "    prefix_string = ''.join(prefix_list)\n",
    "    if not re.fullmatch(r\"((\\?|a):b:(c:)?)*\", prefix_string):\n",
    "        raise ValueError(f\"Illegal response sequence:\\n{prefix_string}\")\n",
    "        \n",
    "    fixed_text_tuples = []\n",
    "    for line in text.split('\\n'):\n",
    "        if line.strip() == \"\": continue\n",
    "        if \":\" not in line: raise ValueError(f\"Illegal response:\\n{stripped_line}\\n{'-'*80}\\nFound in text:\\n{text}\")\n",
    "        prefix = line.split(':')[0]\n",
    "        if prefix not in [\"a\", \"b\", \"c\"]: raise ValueError(f\"Illegal response:\\n{stripped_line}\\n{'-'*80}\\nFound in text:\\n{text}\")\n",
    "            \n",
    "        stripped_line = line[len(prefix)+1:].strip()\n",
    "        if prefix == \"a\" and original_text and stripped_line not in original_text:\n",
    "            warn(f\"The proofreader returned a line of inexact match:\\n{stripped_line}\", UserWarning)\n",
    "            fuzzy_search_result = find_near_matches(stripped_line, original_text, max_l_dist=max_l_dist)\n",
    "            if fuzzy_search_result == []:\n",
    "                prefix = \"?\"\n",
    "                warn(f\"The proofreader returned a line of unknown origin:\\n{stripped_line}\", UserWarning)\n",
    "            else:\n",
    "                stripped_line = fuzzy_search_result[0].matched\n",
    "        fixed_text_tuples.append((prefix, stripped_line))\n",
    "\n",
    "    fixed_text = \"\"\n",
    "    for prefix, stripped_line in fixed_text_tuples:\n",
    "        if prefix in \"?a\": fixed_text += \"\\n\"\n",
    "        fixed_text += f\"{prefix} {stripped_line}\\n\"\n",
    "    fixed_text = f\"\\n\\n{fixed_text.strip()}\"\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8729906-b690-4a20-a3d8-0f54aeb957be",
   "metadata": {},
   "source": [
    "Then, the human would have to go in and read through the file, annotate the selection by square brackets, then perform a single `multiple_replace`.\n",
    "\n",
    "The most important problem we need to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e415532f-ca7f-4283-9cb1-d2f77f52d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(text, replacements):\n",
    "    \"\"\"\n",
    "    Replace multiple substrings in a single pass.\n",
    "\n",
    "    Args:\n",
    "    text (str): The original string.\n",
    "    replacements (dict): A dictionary mapping old substrings to new substrings.\n",
    "\n",
    "    Returns:\n",
    "    str: The modified string after all replacements.\n",
    "    \"\"\"\n",
    "    regex = re.compile(\"|\".join(map(re.escape, replacements.keys())))\n",
    "    return regex.sub(lambda match: replacements[match.group(0)], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7441a-9229-4390-80b4-594e32cdb137",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8042a34-07ed-4188-b84f-3c5ba89e70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "8 chunks\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "# Step 1: Read input_file, query LLM, then write to proofread_file\n",
    "def get_proofread_files(input_file, proofread_file):\n",
    "    with open(input_file, 'r', encoding=\"utf8\") as file:\n",
    "        input_markdown = file.read()\n",
    "    doc = pf.convert_text(input_markdown)\n",
    "    try:\n",
    "        chunks = split_into_chunks(my_filter(doc), max_chars=4_000)\n",
    "        print(f\"{len(chunks)} chunks\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            input_string = '\\n\\n'.join(chunk).strip()\n",
    "            if input_string == \"\": continue\n",
    "            response = gpt_proofread(input_string)\n",
    "            with open(proofread_file, 'a', encoding=\"utf8\") as file:\n",
    "                file.write(process_response(response))\n",
    "    except ValueError as e:\n",
    "        raise e\n",
    "\n",
    "get_proofread_files('index.qmd', 'index_pr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba63f381-1fa9-4fbb-8a60-69218b5a1b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (219380869.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    # ...\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Step 2: After a human has finished proofreading them, perform the substitutions\n",
    "def perform_substitution(input_file, proofread_file, output_file):\n",
    "    # multiple_replace(something)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9ff4927-917c-4863-95c7-49b2786249db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- XZ.\n",
      "+ XZY.\n",
      "?   +\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "print('\\n'.join(list(difflib.ndiff([\"XZ.\"], [\"XZY.\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c3a01-bd51-476e-9ee1-7ebdcfc69a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
